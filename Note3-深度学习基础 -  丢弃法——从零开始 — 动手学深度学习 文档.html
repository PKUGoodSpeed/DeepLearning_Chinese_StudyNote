

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>丢弃法——从零开始 &mdash; 动手学深度学习  文档</title>
  

  
  
    <link rel="shortcut icon" href="../_static/gluon_s2.png"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gluon.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="丢弃法——使用Gluon" href="dropout-gluon.html" />
    <link rel="prev" title="正则化——使用Gluon" href="reg-gluon.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> 动手学深度学习
          

          
            
            <img src="../_static/gluon_white.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_crashcourse/index.html">预备知识</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">深度学习基础</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="shallow-model.html">单层神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-scratch.html">线性回归——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-gluon.html">线性回归——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification.html">分类模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">Softmax回归——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-gluon.html">Softmax回归——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-layer.html">多层神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-scratch.html">多层感知机——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-gluon.html">多层感知机——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="underfit-overfit.html">欠拟合、过拟合和模型选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="reg-scratch.html">正则化——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="reg-gluon.html">正则化——使用Gluon</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">丢弃法——从零开始</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#方法和原理">方法和原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#实现丢弃法">实现丢弃法</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义模型参数">定义模型参数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义模型">定义模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#训练和测试模型">训练和测试模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#小结">小结</a></li>
<li class="toctree-l3"><a class="reference internal" href="#练习">练习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#扫码直达讨论区">扫码直达讨论区</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dropout-gluon.html">丢弃法——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="backprop.html">正向传播和反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="kaggle-gluon-kfold.html">实战Kaggle比赛：预测房价</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gluon-basics/index.html">深度学习计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">卷积神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">循环神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">优化算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gluon-advances/index.html">计算性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">计算机视觉</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing/index.html">自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix/index.html">附录</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">动手学深度学习</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">深度学习基础</a> &raquo;</li>
        
      <li>丢弃法——从零开始</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/chapter_supervised-learning/dropout-scratch.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="丢弃法——从零开始">
<h1>丢弃法——从零开始<a class="headerlink" href="#丢弃法——从零开始" title="永久链接至标题">¶</a></h1>
<p>除了前两节介绍的权重衰减以外，深度学习模型常常使用丢弃法（dropout）来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted
dropout）。它被广泛使用于深度学习。</p>
<div class="section" id="方法和原理">
<h2>方法和原理<a class="headerlink" href="#方法和原理" title="永久链接至标题">¶</a></h2>
<p>为了确保测试模型的确定性，丢弃法的使用只发生在训练模型时，并非测试模型时。当神经网络中的某一层使用丢弃法时，该层的神经元将有一定概率被丢弃掉。设丢弃概率为<span class="math">\(p\)</span>。具体来说，该层任一神经元在应用激活函数后，有<span class="math">\(p\)</span>的概率自乘0，有<span class="math">\(1-p\)</span>的概率自除以<span class="math">\(1-p\)</span>做拉伸。丢弃概率是丢弃法的超参数。</p>
<p>我们在<a class="reference internal" href="multi-layer.html"><span class="doc">“多层神经网络”</span></a>一节的图3.3中描述了一个未使用丢弃法的多层感知机。假设其中隐藏单元<span class="math">\(h_i\)</span>（<span class="math">\(i=1, \ldots, 5\)</span>）的计算表达式为</p>
<div class="math">
\[h_i = \phi(x_1 w_1^{(i)} + x_2 w_2^{(i)} + x_3 w_3^{(i)} + x_4 w_4^{(i)} + b^{(i)}),\]</div>
<p>其中<span class="math">\(\phi\)</span>是激活函数，<span class="math">\(x_1, \ldots, x_4\)</span>是输入，<span class="math">\(w_1^{(i)}, \ldots, w_4^{(i)}\)</span>是权重参数，<span class="math">\(b^{(i)}\)</span>是偏差参数。设丢弃概率为<span class="math">\(p\)</span>，并设随机变量<span class="math">\(\xi_i\)</span>有<span class="math">\(p\)</span>概率为0，有<span class="math">\(1-p\)</span>概率为1。那么，使用丢弃法的隐藏单元<span class="math">\(h_i\)</span>的计算表达式变为</p>
<div class="math">
\[h_i = \frac{\xi_i}{1-p} \phi(x_1 w_1^{(i)} + x_2 w_2^{(i)} + x_3 w_3^{(i)} + x_4 w_4^{(i)} + b^{(i)}).\]</div>
<p>注意到测试模型时不使用丢弃法。由于<span class="math">\(\mathbb{E} (\xi_i) = 1-p\)</span>，同一神经元在模型训练和测试时的输出值的期望不变。</p>
<p>让我们对图3.3中的隐藏层使用丢弃法，一种可能的结果如图3.5所示。</p>
<div class="figure" id="id10">
<img alt="隐藏层使用了丢弃法的多层感知机" src="../_images/dropout.svg" /><p class="caption"><span class="caption-text">隐藏层使用了丢弃法的多层感知机</span></p>
</div>
<p>以图3.5为例，每次训练迭代时，隐藏层中每个神经元都有可能被丢弃，即<span class="math">\(h_i\)</span>（<span class="math">\(i=1, \ldots, 5\)</span>）都有可能为0。因此，输出层每个单元的计算，例如<span class="math">\(o_1 = \phi(h_1 w_1^\prime + h_2 w_2^\prime + h_3 w_3^\prime + h_4 w_4^\prime + h_5 w_5^\prime + b^\prime)\)</span>，都无法过分依赖<span class="math">\(h_1, \ldots, h_5\)</span>中的任一个。这样通常会造成<span class="math">\(o_1\)</span>表达式中的权重参数<span class="math">\(w_1^\prime, \ldots ,w_5^\prime\)</span>都接近0。因此，丢弃法可以起到正则化的作用，并可以用来应对过拟合。</p>
</div>
<div class="section" id="实现丢弃法">
<h2>实现丢弃法<a class="headerlink" href="#实现丢弃法" title="永久链接至标题">¶</a></h2>
<p>根据丢弃法的定义，我们可以很容易地实现丢弃法。下面的<code class="docutils literal"><span class="pre">dropout</span></code>函数将以<code class="docutils literal"><span class="pre">drop_prob</span></code>的概率丢弃NDArray输入<code class="docutils literal"><span class="pre">X</span></code>中的元素。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">gluonbook</span> <span class="kn">as</span> <span class="nn">gb</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">nd</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">loss</span> <span class="k">as</span> <span class="n">gloss</span>

<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">):</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">drop_prob</span> <span class="o">&lt;=</span> <span class="mi">1</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">drop_prob</span>
    <span class="c1"># 这种情况下把全部元素都丢弃。</span>
    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">()</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">keep_prob</span>
    <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">X</span> <span class="o">/</span> <span class="n">keep_prob</span>
</pre></div>
</div>
</div>
<p>我们运行几个例子来验证一下<code class="docutils literal"><span class="pre">dropout</span></code>函数。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[2]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>

<span></span>[[  0.   1.   2.   3.]
 [  4.   5.   6.   7.]
 [  8.   9.  10.  11.]
 [ 12.  13.  14.  15.]
 [ 16.  17.  18.  19.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[3]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>

<span></span>[[  0.   0.   0.   6.]
 [  0.  10.   0.   0.]
 [ 16.  18.  20.   0.]
 [ 24.  26.   0.   0.]
 [  0.  34.   0.   0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[4]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>

<span></span>[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="定义模型参数">
<h2>定义模型参数<a class="headerlink" href="#定义模型参数" title="永久链接至标题">¶</a></h2>
<p>实验中，我们依然使用<a class="reference external" href="../chapter_crashcourse/softmax-regression-scratch.md">“Softmax回归——从零开始”</a>一节中介绍的Fashion-MNIST数据集。我们将定义一个包含两个隐藏层的多层感知机。其中两个隐藏层的输出个数都是256。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_hiddens1</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">num_hiddens2</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">W1</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens1</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens1</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_hiddens1</span><span class="p">,</span> <span class="n">num_hiddens2</span><span class="p">))</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens2</span><span class="p">)</span>
<span class="n">W3</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_hiddens2</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">))</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">]</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
    <span class="n">param</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="定义模型">
<h2>定义模型<a class="headerlink" href="#定义模型" title="永久链接至标题">¶</a></h2>
<p>我们的模型就是将全连接层和激活函数ReLU串起来，并对激活函数的输出使用丢弃法。我们可以分别设置各个层的丢弃概率。通常，建议把靠近输入层的丢弃概率设的小一点。在这个实验中，我们把第一个隐藏层的丢弃概率设为0.2，把第二个隐藏层的丢弃概率设为0.5。我们只需在训练模型时使用丢弃法。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">drop_prob1</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">drop_prob2</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">))</span>
    <span class="n">H1</span> <span class="o">=</span> <span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
    <span class="c1"># 只在训练模型时使用丢弃法。</span>
    <span class="k">if</span> <span class="n">autograd</span><span class="o">.</span><span class="n">is_training</span><span class="p">():</span>
        <span class="c1"># 在第一层全连接后添加丢弃层。</span>
        <span class="n">H1</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">H1</span><span class="p">,</span> <span class="n">drop_prob1</span><span class="p">)</span>
    <span class="n">H2</span> <span class="o">=</span> <span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">autograd</span><span class="o">.</span><span class="n">is_training</span><span class="p">():</span>
        <span class="c1"># 在第二层全连接后添加丢弃层。</span>
        <span class="n">H2</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">H2</span><span class="p">,</span> <span class="n">drop_prob2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nd</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H2</span><span class="p">,</span> <span class="n">W3</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="训练和测试模型">
<h2>训练和测试模型<a class="headerlink" href="#训练和测试模型" title="永久链接至标题">¶</a></h2>
<p>这部分和之前多层感知机的训练与测试类似。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-python"><div class="highlight"><pre>
<span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">gb</span><span class="o">.</span><span class="n">train_cpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span>
             <span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch 1, loss 1.1198, train acc 0.570, test acc 0.780
epoch 2, loss 0.5707, train acc 0.790, test acc 0.819
epoch 3, loss 0.4830, train acc 0.824, test acc 0.803
epoch 4, loss 0.4426, train acc 0.838, test acc 0.858
epoch 5, loss 0.4154, train acc 0.848, test acc 0.854
</pre></div></div>
</div>
</div>
<div class="section" id="小结">
<h2>小结<a class="headerlink" href="#小结" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>我们可以通过使用丢弃法应对过拟合。</li>
<li>只需在训练模型时使用丢弃法。</li>
</ul>
</div>
<div class="section" id="练习">
<h2>练习<a class="headerlink" href="#练习" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>尝试不使用丢弃法，看看这个包含两个隐藏层的多层感知机可以得到什么结果。</li>
<li>如果把本节中的两个丢弃概率超参数对调，会有什么结果？</li>
</ul>
</div>
<div class="section" id="扫码直达讨论区">
<h2>扫码直达<a class="reference external" href="https://discuss.gluon.ai/t/topic/1278">讨论区</a><a class="headerlink" href="#扫码直达讨论区" title="永久链接至标题">¶</a></h2>
<div class="figure">
<img alt="" src="../_images/qr_dropout-scratch.svg" /></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dropout-gluon.html" class="btn btn-neutral float-right" title="丢弃法——使用Gluon" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="reg-gluon.html" class="btn btn-neutral" title="正则化——使用Gluon" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'zh_CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/baidu_tongji.js"></script>
      <script type="text/javascript" src="../_static/google_analytics.js"></script>
      <script type="text/javascript" src="../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>