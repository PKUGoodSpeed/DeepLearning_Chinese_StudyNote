

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>正向传播和反向传播 &mdash; 动手学深度学习  文档</title>
  

  
  
    <link rel="shortcut icon" href="../_static/gluon_s2.png"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gluon.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="实战Kaggle比赛：预测房价" href="kaggle-gluon-kfold.html" />
    <link rel="prev" title="丢弃法——使用Gluon" href="dropout-gluon.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> 动手学深度学习
          

          
            
            <img src="../_static/gluon_white.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_crashcourse/index.html">预备知识</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">深度学习基础</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="shallow-model.html">单层神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-scratch.html">线性回归——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-gluon.html">线性回归——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification.html">分类模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">Softmax回归——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-gluon.html">Softmax回归——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi-layer.html">多层神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-scratch.html">多层感知机——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="mlp-gluon.html">多层感知机——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="underfit-overfit.html">欠拟合、过拟合和模型选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="reg-scratch.html">正则化——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="reg-gluon.html">正则化——使用Gluon</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropout-scratch.html">丢弃法——从零开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropout-gluon.html">丢弃法——使用Gluon</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">正向传播和反向传播</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#概念">概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#案例分析——正则化的多层感知机">案例分析——正则化的多层感知机</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#定义模型">定义模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#模型计算图">模型计算图</a></li>
<li class="toctree-l4"><a class="reference internal" href="#正向传播">正向传播</a></li>
<li class="toctree-l4"><a class="reference internal" href="#反向传播">反向传播</a></li>
<li class="toctree-l4"><a class="reference internal" href="#正向传播和反向传播相互依赖">正向传播和反向传播相互依赖</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#小结">小结</a></li>
<li class="toctree-l3"><a class="reference internal" href="#练习">练习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#扫码直达讨论区">扫码直达讨论区</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kaggle-gluon-kfold.html">实战Kaggle比赛：预测房价</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gluon-basics/index.html">深度学习计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">卷积神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">循环神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">优化算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gluon-advances/index.html">计算性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">计算机视觉</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing/index.html">自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix/index.html">附录</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">动手学深度学习</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">深度学习基础</a> &raquo;</li>
        
      <li>正向传播和反向传播</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/chapter_supervised-learning/backprop.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="正向传播和反向传播">
<h1>正向传播和反向传播<a class="headerlink" href="#正向传播和反向传播" title="永久链接至标题">¶</a></h1>
<p>我们一直使用优化算法来训练深度学习模型，例如小批量随机梯度下降。实际上，优化算法通常都会依赖模型参数梯度的计算。然而，在深度学习模型中，由于网络结构的复杂性，模型参数梯度的计算往往并不直观。虽然我们可以通过MXNet轻松获取模型参数的梯度，但了解它们的计算将有助于我们进一步理解深度学习模型训练的本质。</p>
<div class="section" id="概念">
<h2>概念<a class="headerlink" href="#概念" title="永久链接至标题">¶</a></h2>
<p>反向传播（back-propagation）是计算神经网络参数梯度的方法。总的来说，反向传播中会依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储损失函数有关神经网络各层的中间变量以及参数的梯度。</p>
<p>反向传播对于各层变量和参数的梯度计算可能会依赖各层变量和参数的当前值。对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型中间变量的过程叫做正向传播（forward
propagation）。</p>
</div>
<div class="section" id="案例分析——正则化的多层感知机">
<h2>案例分析——正则化的多层感知机<a class="headerlink" href="#案例分析——正则化的多层感知机" title="永久链接至标题">¶</a></h2>
<p>为了解释正向传播和反向传播，我们以一个简单的<span class="math">\(L_2\)</span>范数正则化的多层感知机为例。</p>
<div class="section" id="定义模型">
<h3>定义模型<a class="headerlink" href="#定义模型" title="永久链接至标题">¶</a></h3>
<p>考虑类别数为<span class="math">\(y\)</span>的分类问题。给定一个特征为<span class="math">\(\boldsymbol{x} \in \mathbb{R}^x\)</span>和标签为离散值<span class="math">\(y^\prime\)</span>的训练数据样本。不考虑偏差项，我们可以得到中间变量</p>
<div class="math">
\[\boldsymbol{z} = \boldsymbol{W}^{(1)} \boldsymbol{x},\]</div>
<p>其中<span class="math">\(\boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times x}\)</span>是模型参数。中间变量<span class="math">\(\boldsymbol{z} \in \mathbb{R}^h\)</span>应用按元素操作的激活函数<span class="math">\(\phi\)</span>后将得到向量长度为<span class="math">\(h\)</span>的隐藏层变量</p>
<div class="math">
\[\boldsymbol{h} = \phi (\boldsymbol{z}).\]</div>
<p>隐藏层<span class="math">\(\boldsymbol{h} \in \mathbb{R}^h\)</span>也是一个中间变量。通过模型参数<span class="math">\(\boldsymbol{W}^{(2)} \in \mathbb{R}^{y \times h}\)</span>可以得到向量长度为<span class="math">\(y\)</span>的输出层变量</p>
<div class="math">
\[\boldsymbol{o} = \boldsymbol{W}^{(2)} \boldsymbol{h}.\]</div>
<p>假设损失函数为<span class="math">\(\ell\)</span>，我们可以计算出单个数据样本的损失项</p>
<div class="math">
\[L = \ell(\boldsymbol{o}, y^\prime).\]</div>
<p>根据<span class="math">\(L_2\)</span>范数正则化的定义，给定超参数<span class="math">\(\lambda\)</span>，正则化项即</p>
<div class="math">
\[s = \frac{\lambda}{2} (\|\boldsymbol{W}^{(1)}\|_F^2 + \|\boldsymbol{W}^{(2)}\|_F^2),\]</div>
<p>其中每个矩阵Frobenius范数的平方项即该矩阵元素的平方和。最终，模型在给定的数据样本上带正则化的损失为</p>
<div class="math">
\[J = L + s.\]</div>
<p>我们将<span class="math">\(J\)</span>叫做有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。</p>
</div>
<div class="section" id="模型计算图">
<h3>模型计算图<a class="headerlink" href="#模型计算图" title="永久链接至标题">¶</a></h3>
<p>为了可视化模型变量和参数之间在计算中的依赖关系，我们可以绘制模型计算图，如图3.6所示。例如，正则化项<span class="math">\(s\)</span>的计算依赖模型参数<span class="math">\(\boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\boldsymbol{W}^{(2)}\)</span>。</p>
<div class="figure" id="id12">
<img alt="模型计算中的依赖关系" src="../_images/backprop.svg" /><p class="caption"><span class="caption-text">模型计算中的依赖关系</span></p>
</div>
</div>
<div class="section" id="正向传播">
<h3>正向传播<a class="headerlink" href="#正向传播" title="永久链接至标题">¶</a></h3>
<p>在反向传播计算梯度之前，我们先做一次正向传播。也就是说，按照图3.6中箭头顺序，并根据模型参数的当前值，依次计算并存储模型中各个中间变量的值。例如，在计算损失项<span class="math">\(L\)</span>之前，我们需要依次计算并存储<span class="math">\(\boldsymbol{z}, \boldsymbol{h}, \boldsymbol{o}\)</span>的值。</p>
</div>
<div class="section" id="反向传播">
<h3>反向传播<a class="headerlink" href="#反向传播" title="永久链接至标题">¶</a></h3>
<p>刚刚提到，图3.6中模型的参数是<span class="math">\(\boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\boldsymbol{W}^{(2)}\)</span>。根据<a class="reference internal" href="shallow-model.html"><span class="doc">”单层神经网络”</span></a>一节中定义的小批量随机梯度下降，我们需要对小批量中每个样本目标函数<span class="math">\(J\)</span>关于<span class="math">\(\boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\boldsymbol{W}^{(2)}\)</span>的梯度求平均来迭代<span class="math">\(\boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\boldsymbol{W}^{(2)}\)</span>。也就是说，每一次迭代都需要计算模型参数梯度<span class="math">\(\partial J/\partial \boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\partial J/\partial \boldsymbol{W}^{(2)}\)</span>。根据图3.6中的依赖关系，我们可以按照其中箭头所指的反方向依次计算并存储梯度。</p>
<p>为了表述方便，对输入输出<span class="math">\(\mathsf{X}, \mathsf{Y}, \mathsf{Z}\)</span>为任意形状张量的函数<span class="math">\(\mathsf{Y}=f(\mathsf{X})\)</span>和<span class="math">\(\mathsf{Z}=g(\mathsf{Y})\)</span>，我们使用</p>
<div class="math">
\[\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}})\]</div>
<p>来表达链式法则。</p>
<p>首先，我们计算目标函数有关损失项和有关正则项的梯度</p>
<div class="math">
\[\frac{\partial J}{\partial L} = 1,\]</div>
<div class="math">
\[\frac{\partial J}{\partial s} = 1.\]</div>
<p>其次，我们依据链式法则计算目标函数有关输出层变量的梯度<span class="math">\(\partial J/\partial \boldsymbol{o} \in \mathbb{R}^{y}\)</span>：</p>
<div class="math">
\[\frac{\partial J}{\partial \boldsymbol{o}}
= \text{prod}(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \boldsymbol{o}})
= \frac{\partial L}{\partial \boldsymbol{o}}.\]</div>
<p>正则项有关两个参数的梯度可以很直观地计算：</p>
<div class="math">
\[\frac{\partial s}{\partial \boldsymbol{W}^{(1)}} = \lambda \boldsymbol{W}^{(1)},\]</div>
<div class="math">
\[\frac{\partial s}{\partial \boldsymbol{W}^{(2)}} = \lambda \boldsymbol{W}^{(2)}.\]</div>
<p>现在我们可以计算最靠近输出层的模型参数的梯度<span class="math">\(\partial J/\partial \boldsymbol{W}^{(2)} \in \mathbb{R}^{y \times h}\)</span>。在图3.6中，
<span class="math">\(J\)</span>分别通过<span class="math">\(\boldsymbol{o}\)</span>和<span class="math">\(s\)</span>依赖<span class="math">\(\boldsymbol{W}^{(2)}\)</span>。依据链式法则，我们有</p>
<div class="math">
\[\frac{\partial J}{\partial \boldsymbol{W}^{(2)}}
= \text{prod}(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{W}^{(2)}}) + \text{prod}(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(2)}})
= \frac{\partial J}{\partial \boldsymbol{o}} \boldsymbol{h}^\top + \lambda \boldsymbol{W}^{(2)}.\]</div>
<p>沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度<span class="math">\(\partial J/\partial \boldsymbol{h} \in \mathbb{R}^h\)</span>可以这样计算：</p>
<div class="math">
\[\frac{\partial J}{\partial \boldsymbol{h}}
= \text{prod}(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{h}})
= {\boldsymbol{W}^{(2)}}^\top \frac{\partial J}{\partial \boldsymbol{o}}.\]</div>
<p>注意到激活函数<span class="math">\(\phi\)</span>是按元素操作的，中间变量<span class="math">\(\boldsymbol{z}\)</span>的梯度<span class="math">\(\partial J/\partial \boldsymbol{z} \in \mathbb{R}^h\)</span>的计算需要使用按元素乘法符<span class="math">\(\odot\)</span>：</p>
<div class="math">
\[\frac{\partial J}{\partial \boldsymbol{z}}
= \text{prod}(\frac{\partial J}{\partial \boldsymbol{h}}, \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}})
= \frac{\partial J}{\partial \boldsymbol{h}} \odot \phi^\prime(\boldsymbol{z}).\]</div>
<p>最终，我们可以得到最靠近输入层的模型参数的梯度<span class="math">\(\partial J/\partial \boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times x}\)</span>。在图3.6中，<span class="math">\(J\)</span>分别通过<span class="math">\(\boldsymbol{z}\)</span>和<span class="math">\(s\)</span>依赖<span class="math">\(\boldsymbol{W}^{(1)}\)</span>。依据链式法则，我们有</p>
<div class="math">
\[\frac{\partial J}{\partial \boldsymbol{W}^{(1)}}
= \text{prod}(\frac{\partial J}{\partial \boldsymbol{z}}, \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}^{(1)}}) + \text{prod}(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(1)}})
= \frac{\partial J}{\partial \boldsymbol{z}} \boldsymbol{x}^\top + \lambda \boldsymbol{W}^{(1)}.\]</div>
<p>需要再次提醒的是，每次迭代中，上述各个依次计算出的梯度会被依次存储或更新。这是为了避免重复计算。例如，由于输出层变量梯度<span class="math">\(\partial J/\partial \boldsymbol{o}\)</span>被计算存储，反向传播稍后的参数梯度<span class="math">\(\partial J/\partial \boldsymbol{W}^{(2)}\)</span>和隐藏层变量梯度<span class="math">\(\partial J/\partial \boldsymbol{h}\)</span>的计算可以直接读取输出层变量梯度的值，而无需重复计算。</p>
</div>
<div class="section" id="正向传播和反向传播相互依赖">
<h3>正向传播和反向传播相互依赖<a class="headerlink" href="#正向传播和反向传播相互依赖" title="永久链接至标题">¶</a></h3>
<p>事实上，正向传播和反向传播相互依赖。</p>
<p>一方面，正向传播的计算可能依赖模型参数的当前值。而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，图3.6中，计算正则化项<span class="math">\(s\)</span>依赖模型参数<span class="math">\(\boldsymbol{W}^{(1)}\)</span>和<span class="math">\(\boldsymbol{W}^{(2)}\)</span>的当前值。而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。</p>
<p>另一方面，反向传播的梯度计算可能依赖各变量的当前值。而这些变量的当前值是通过正向传播计算的。举例来说，参数梯度<span class="math">\(\partial J/\partial \boldsymbol{W}^{(2)}\)</span>的计算需要依赖隐藏层变量的当前值<span class="math">\(\boldsymbol{h}\)</span>。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。</p>
<p>因此，在模型参数初始化完成后，我们可以交替进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。</p>
</div>
</div>
<div class="section" id="小结">
<h2>小结<a class="headerlink" href="#小结" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>反向传播沿着从输出层到输入层的顺序，依次计算并存储神经网络中间变量和参数的梯度。</li>
<li>正向传播沿着从输入层到输出层的顺序，依次计算并存储神经网络的中间变量。</li>
<li>正向传播和反向传播相互依赖。</li>
</ul>
</div>
<div class="section" id="练习">
<h2>练习<a class="headerlink" href="#练习" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>学习了本节内容后，你是否能解释<a class="reference internal" href="multi-layer.html"><span class="doc">“多层神经网络”</span></a>一节中提到的层数较多时梯度可能会衰减或爆炸的原因？</li>
</ul>
</div>
<div class="section" id="扫码直达讨论区">
<h2>扫码直达<a class="reference external" href="https://discuss.gluon.ai/t/topic/3710">讨论区</a><a class="headerlink" href="#扫码直达讨论区" title="永久链接至标题">¶</a></h2>
<div class="figure">
<img alt="" src="../_images/qr_backprop.svg" /></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="kaggle-gluon-kfold.html" class="btn btn-neutral float-right" title="实战Kaggle比赛：预测房价" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dropout-gluon.html" class="btn btn-neutral" title="丢弃法——使用Gluon" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'zh_CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/baidu_tongji.js"></script>
      <script type="text/javascript" src="../_static/google_analytics.js"></script>
      <script type="text/javascript" src="../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>